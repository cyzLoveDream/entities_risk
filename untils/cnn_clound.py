# -*- coding : utf-8 -*-import pandas as pdimport jiebaimport warningsimport osimport refrom keras.layers.core import Activation, Dense,Dropoutfrom keras.layers.embeddings import Embeddingfrom keras.layers.recurrent import LSTMfrom keras.models import Sequentialfrom keras.preprocessing import sequencefrom keras.utils import np_utilsfrom keras.models import load_modelfrom keras.callbacks import ModelCheckpointfrom keras.callbacks import EarlyStoppingfrom keras.callbacks import LearningRateSchedulerfrom keras.callbacks import ReduceLROnPlateaufrom sklearn.model_selection import train_test_splitimport keras.backend as Kimport collections  #用来统计词频import numpy as npimport sysimport tensorflow as tffrom tqdm import  tqdmimport argparseimport timewarnings.filterwarnings("ignore")FLAGS = None# os.chdir("E:\\ccf\\Entity_Extraction")# 设置神经网络参数embedding_size = 500hidden_layer_size = 128batch_size = 32epoch = 5max_sentence_length = 850def load_train_data(filename,data_shape):	"""	解析训练数据	:param filename:	:return:	"""	data = []	with tf.Session() as sess:		# file_name = "raw_data\\train_tf.tfrecords"		filename_queue = tf.train.string_input_producer([filename],shuffle=False,seed=0)		read = tf.TFRecordReader()		_,serialized_example = read.read(filename_queue)				features = tf.parse_single_example(serialized_example,		                                   features={			                                   "label": tf.FixedLenFeature([],tf.int64),			                                   "digest": tf.FixedLenFeature([],tf.string),			                                   "eventLevel": tf.FixedLenFeature([],tf.string),			                                   "keywords": tf.FixedLenFeature([],tf.string),			                                   "name": tf.FixedLenFeature([],tf.string),			                                   })				label = tf.cast(features['label'],tf.int64)		digest = features['digest']		name = features["name"]		keyword = features['keywords']		eventLevel = features['eventLevel']		init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())		sess.run(init_op)		coord = tf.train.Coordinator()		threads = tf.train.start_queue_runners(coord=coord)		# 14141		for i in range(data_shape):			content_list = sess.run([name,digest,keyword,eventLevel,label])			c_l = []			for d in content_list[0:-1]:				c_l.append(str(d,"utf-8"))			c_l.append(content_list[-1])			data.append(c_l)		# print(label.eval())		coord.request_stop()		coord.join(threads)		sess.close()	return datadef user_dicts(input_filename):	"""	解析字典	:param input_filename:	:return:	"""	data = []	with tf.Session() as sess:		# file_name = "raw_data\\train_tf.tfrecords"		filename_queue = tf.train.string_input_producer([input_filename],shuffle=False,seed=0)		read = tf.TFRecordReader()		_,serialized_example = read.read(filename_queue)		features = tf.parse_single_example(serialized_example,		                                   features={			                                   "line": tf.FixedLenFeature([],tf.string),			                                   })		line = features['line']		init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())		sess.run(init_op)		coord = tf.train.Coordinator()		threads = tf.train.start_queue_runners(coord=coord)		for i in range(339):			# labels, digests, names, keywords,eventLevels = sess.run([label, digest, name, keyword, eventLevel])			# print(labels, digests,names,keywords,eventLevels)			lines = str(sess.run(line),"utf-8")			data.append(lines)		# print(label.eval())		coord.request_stop()		coord.join(threads)		sess.close()	return datadef parse_test_data(filename, data_shape):	"""	解析测试数据	:param filename:	:return:	"""	data_list = []	with tf.Session() as sess:		# file_name = "raw_data\\train_new.tfrecords"		filename_queue = tf.train.string_input_producer([filename],shuffle=False,seed=0)		read = tf.TFRecordReader()		_,serialized_example = read.read(filename_queue)				features = tf.parse_single_example(serialized_example,		                                   features={			                                   "digest": tf.FixedLenFeature([],tf.string),			                                   "name": tf.FixedLenFeature([],tf.string),			                                   })		digest = features['digest']		name = features["name"]		init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())		sess.run(init_op)		coord = tf.train.Coordinator()		threads = tf.train.start_queue_runners(coord=coord)		for i in range(data_shape):			content_list = sess.run([name,digest])			c_l = []			c_l.append(str(content_list[0],"utf-8"))			c_l.append(str(content_list[1],"utf-8"))			data_list.append(c_l)		coord.request_stop()		coord.join(threads)		sess.close()	return data_listdef load_test_data(filename,data_shape):	"""	载入需要预测的数据	:param filename:	:return:	"""	data_list = parse_test_data(filename,data_shape)	data_pd = pd.DataFrame(data_list,columns=["name","digest"])	# 利用摘要进行实体的情感预测	# print(data_pd.head())	return data_pddef dataframe_to_tfrecords(data, output_filename):	data1 = np.array(data)	data_list = data1.tolist()	start_time = time.time()	writer = tf.python_io.TFRecordWriter(output_filename)	for d in tqdm(data_list):		name = [bytes(str(d[0]),"utf-8")]		digest = [bytes(str(d[1]),"utf-8")]		label = [d[2]]		example = tf.train.Example(features=tf.train.Features(feature={			"name":				tf.train.Feature(bytes_list=tf.train.BytesList(value=name)),			"digest":				tf.train.Feature(bytes_list = tf.train.BytesList(value=digest)),			"label":				tf.train.Feature(int64_list=tf.train.Int64List(value=label))			}))		writer.write(example.SerializeToString())  # 序列化为字符串	writer.close()	print("Successfully convert to {}".format(output_filename))	end_time = time.time()	print("\nThe pretraining process ran for {0} minutes\n".format((end_time - start_time) / 60))def f1(y_true,y_pred):	def recall(y_true,y_pred):		"""Recall metric.		Only computes a batch-wise average of recall.		Computes the recall, a metric for multi-label classification of		how many relevant items are selected.		"""		true_positives = K.sum(K.round(K.clip(y_true * y_pred,0,1)))		possible_positives = K.sum(K.round(K.clip(y_true,0,1)))		recall = true_positives / (possible_positives + K.epsilon())		return recall		def precision(y_true,y_pred):		"""Precision metric.		Only computes a batch-wise average of precision.		Computes the precision, a metric for multi-label classification of		how many selected items are relevant.		"""		true_positives = K.sum(K.round(K.clip(y_true * y_pred,0,1)))		predicted_positives = K.sum(K.round(K.clip(y_pred,0,1)))		precision = true_positives / (predicted_positives + K.epsilon())		return precision		precisions = precision(y_true,y_pred)	recalls = recall(y_true,y_pred)	return 2 * ((precisions * recalls) / (precisions + recalls))def nn(vocab_size, ckpt_path):	model = Sequential()	model.add(Embedding(vocab_size,embedding_size,input_length=max_sentence_length))	model.add(LSTM(hidden_layer_size,dropout=0.2,recurrent_dropout=0.2))	model.add(Dense(32,activation="relu"))	model.add(Dropout(0.5))	model.add(Dense(16,activation="relu"))	model.add(Dropout(0.5))	model.add(Dense(3,activation='softmax'))	print(model.summary())	try:		model.load_weights(ckpt_path)		print("load weights finish....")	except:		print("no pre-weights...")		pass	model.compile(loss="binary_crossentropy",optimizer="adam",metrics=["accuracy",f1])	return modeldef train_model(vocab_size, x_train, y_train, x_test, y_test,embedding_matrix = False):	"""	训练的模型	:param vocab_size: 文本的词表的长度	:param x_train: 训练集	:param y_train: 训练集label	:param x_test:  测试集	:param y_test:  测试集label	:return: ckpt_path 最好权重路径	:return: cloud_ckpt_path 最好权重路径（tf版）	:return: model_train_file 最终模型路径	:return: model_file 最终模型路径（tf版）	"""	# 最好权重保存路径、用于callback调用	# ckpt = os.path.join(FLAGS.tb_dir,"entity.best.hdf5")	ckpt = "../entity.best.hdf5"	# ckpt_file = os.path.join(FLAGS.data_dir,"entity.best.hdf5")	# 预训练模型保存的地址	# m_path = os.path.join(FLAGS.tb_dir,"entity_model.h5py")	m_path = "../entity_model.h5py"	# model_file = os.path.join(FLAGS.data_dir,"entity_model.h5py")	callback = ModelCheckpoint(ckpt,verbose=1,save_best_only=True,save_weights_only=True)	early_stop = EarlyStopping(monitor='val_loss',patience=5,verbose=1)	reduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.2,	                              patience=5,min_lr=0.001)	callback_list = [callback,early_stop,reduce_lr]	model = nn(vocab_size, ckpt)	print("create model finish...")	model.fit(x_train,y_train,	          batch_size=batch_size,	          callbacks=callback_list,	          shuffle=True,	          epochs=epoch,	          verbose=1,	          validation_data=(x_test,y_test))	model.save(m_path)	return ckptdef train():	# 基本数据	# train_file_path = os.path.join(FLAGS.data_dir,"train.tfrecords")	train_file_path ="../train.tfrecords"	test_file_path = "../test_11.tfrecords"	user_dict_filename = "../user_dict.tfrecords"	sub_path = "../submission.tfrecords"	# 导入数据	print("begin load data")	data = load_train_data(train_file_path,14141)	test_data_pd = load_test_data(test_file_path,16913)	print("the test data's shape is:", test_data_pd.shape)	print("load data finish")	data_pd = pd.DataFrame(data,columns=["name","digest","keyword","eventLevel","label"])	# 导入数据完成	data_pd = data_pd[["name","digest","label"]]	all_data_pd = pd.concat([data_pd,test_data_pd])	contents = all_data_pd.digest.values	contents_list = []	maxlen = 0	word_freqs = collections.Counter()  # 词频	num_recs = 0  # 样本数	print("begin load user_dict")	user_dict = user_dicts(user_dict_filename)	for w in user_dict:		jieba.add_word(w)	print("finish load dict")	print("begin handle data")	for c in contents:		#     获取句子的最大长度		if len(c) > maxlen:			maxlen = len(c)		cut_list = jieba.lcut(c)		fenci_list_without_biaodian = []		for fl in cut_list:			pattern = re.compile(u"[\u4e00-\u9fa5]+")			result = re.findall(pattern,fl)			if result != "":				fenci_list_without_biaodian.extend(result)			#     分完词之后进行词频统计		for word in fenci_list_without_biaodian:			word_freqs[word] += 1		num_recs += 1		contents_list.append(fenci_list_without_biaodian)	print("the all words count is {0}, the max_len of sample is {1}, the samples count is {2}".format(len(word_freqs),maxlen,num_recs))	print("the train's shape is {0}, the all data's shape is {1}".format(data_pd.shape,all_data_pd.shape))	max_features = 23500	print("set up index")	# 建立索引	vocab_size = min(max_features,len(word_freqs)) + 2	word2index = {x[0]: i + 2 for i,x in enumerate(word_freqs.most_common(max_features))}	word2index["PAD"] = 0	word2index["UNK"] = 1	index2word = {v: k for k,v in word2index.items()}	X = np.empty(num_recs,dtype=list)	label = []	labels = data_pd.label.values	print("the labels is:",labels)	print("the row label's length is:",len(labels))	print("begin covert text to index")	for i in tqdm(range(len(contents))):		cut_list = jieba.lcut(contents[i])		fenci_list_without_biaodian = []		for fl in cut_list:			pattern = re.compile(u"[\u4e00-\u9fa5]+")			result = re.findall(pattern,fl)			if result != "":				fenci_list_without_biaodian.extend(result)		seqs = []		for w in fenci_list_without_biaodian:			if w in word2index:				seqs.append(word2index[w])			else:				seqs.append(word2index["UNK"])		X[i] = seqs		if i < data_pd.shape[0]:			label.append(labels[i])	X = sequence.pad_sequences(X,maxlen=max_sentence_length)	print("label's length",len(label))	label = np.array(label)	print("the label's shape is",label.shape)	print("split data")	train = X[:data_pd.shape[0]]	test = X[data_pd.shape[0]:]	print("the train's shape is {0}, the test's shape is {1}".format(train.shape,test.shape))	x_train,x_test,y_train,y_test = train_test_split(train,label,test_size=0.2,random_state=42)	print('Shape of x_train tensor:',x_train.shape)	print('Shape of y_train tensor:',y_train.shape)	print('Shape of x_test tensor:',x_test.shape)	print('Shape of y_test tensor:',y_test.shape)	y_train = np_utils.to_categorical(y_train,3)	y_test = np_utils.to_categorical(y_test,3)	print("begin trainning.....")	ckpt_path = train_model(vocab_size,x_train,y_train,x_test,y_test)	model = nn(vocab_size,ckpt_path)	print("evaluate data..")	print("begin predict")	pred = model.predict_classes(test,verbose=1)	print(pred)	print("the pred's shape is: ",pred.shape)	test_data_pd["pred"] = pred	print("predict finish...")	dataframe_to_tfrecords(test_data_pd, sub_path)# 	print("the train time is {0}".format(time.time()- now))	print("finish")		def main():	# evalmodel()	train()if __name__ == '__main__':		main()